{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics import classification as C\n",
    "from nn_ood.posteriors import SCOD\n",
    "from nn_ood.distributions import CategoricalLogit\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.num_classes = 10\n",
    "\n",
    "        # mnist images are (1, 28, 28) (channels, width, height)\n",
    "        self.layers = nn.Sequential(*[\n",
    "            nn.Conv2d(1, 8, 3, 1), # (b, 8, 26, 26)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 16, 5, 1), # (b, 16, 22, 22)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # (b, 16, 11, 11)\n",
    "            nn.Flatten(), # (b, 16*11*11)\n",
    "            nn.Linear(16*11*11, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 10),\n",
    "        ])\n",
    "\n",
    "        self.metrics = nn.ModuleDict({\n",
    "            'acc': C.MulticlassAccuracy(10),\n",
    "            'f1': C.MulticlassF1Score(10, average='micro'),\n",
    "        })\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, 1, 28, 28)\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:09<00:00, 98.91it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Test loss: 0.0000, Test accuracy: 0.9857\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "def train(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(tqdm(train_loader)):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def test(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    acc = correct / len(test_loader.dataset)\n",
    "    return test_loss, acc\n",
    "\n",
    "model = Net()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST('data', train=True, download=True,\n",
    "                               transform=torchvision.transforms.Compose([\n",
    "                                   torchvision.transforms.ToTensor(),\n",
    "                                   torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "                               ])),\n",
    "    batch_size=64, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST('data', train=False, transform=torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])),\n",
    "    batch_size=1000, shuffle=True)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1):\n",
    "    print(f'Epoch {epoch}')\n",
    "    train(model, train_loader, optimizer, criterion, device)\n",
    "    test_loss, acc = test(model, test_loader, criterion, device)\n",
    "    print(f'Epoch {epoch}: Test loss: {test_loss:.4f}, Test accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing basis\n",
      "using T = 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:46<00:00, 93.88it/s]\n",
      "/home/wayne/CS5340_Uncertainty_Modelling_in_AI/SCOD/nn_ood/sketching.py:123: UserWarning: torch.triangular_solve is deprecated in favor of torch.linalg.solve_triangularand will be removed in a future PyTorch release.\n",
      "torch.linalg.solve_triangular has its arguments reversed and does not return a copy of one of the inputs.\n",
      "X = torch.triangular_solve(B, A).solution\n",
      "should be replaced with\n",
      "X = torch.linalg.solve_triangular(A, B). (Triggered internally at ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2192.)\n",
      "  X,_ = torch.triangular_solve(U.t() @ self.W, T) # (k, N)\n"
     ]
    }
   ],
   "source": [
    "mnist_test = torchvision.datasets.MNIST('data', train=False, transform=torchvision.transforms.Compose([\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "            ]))\n",
    "\n",
    "wrapped_model = SCOD(model, CategoricalLogit())\n",
    "wrapped_model.process_dataset(mnist_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 10]) torch.Size([1000])\n",
      "tensor([1.4033e+00, 4.4059e-01, 2.3828e-01, 1.2598e+00, 4.8967e+00, 2.6099e-01,\n",
      "        1.1167e+00, 2.7186e-01, 2.2019e+00, 3.8087e-01, 2.0626e+00, 6.4375e-02,\n",
      "        2.6413e-01, 1.5518e+00, 1.0360e+00, 8.2823e-01, 2.2509e+01, 1.1224e+00,\n",
      "        2.0028e-01, 7.6915e-01, 2.7657e-01, 1.6222e+00, 1.8835e+01, 1.8802e-01,\n",
      "        1.2063e+00, 4.4656e-01, 2.2343e+00, 1.3389e+00, 1.2773e+00, 7.2326e+00,\n",
      "        1.2717e+01, 1.6522e+01, 2.8836e-01, 4.7021e-01, 2.0457e+00, 1.9605e+01,\n",
      "        5.1310e-01, 5.6402e+00, 9.9181e-02, 6.3184e-01, 1.5551e+00, 4.6970e-01,\n",
      "        2.3044e-01, 1.3637e+00, 1.7511e+00, 4.3291e-01, 8.4122e+00, 1.3451e+00,\n",
      "        9.4520e-01, 3.1253e-01, 8.9526e-01, 3.0207e-01, 1.2391e+00, 6.0206e+00,\n",
      "        6.6202e-01, 5.9056e-01, 1.0526e+01, 9.8520e-01, 2.3890e+00, 1.9355e+00,\n",
      "        4.5551e-01, 3.8177e+00, 3.9123e-01, 6.5065e-01, 1.4731e+01, 1.9901e+01,\n",
      "        2.8396e-01, 1.0213e+00, 9.4839e+00, 8.7186e-01, 9.2660e+00, 1.4080e+00,\n",
      "        5.8575e-01, 2.4688e-01, 2.2868e+01, 5.5651e-02, 4.7821e-01, 3.1491e-01,\n",
      "        1.2752e-01, 9.0333e-01, 4.0028e-01, 2.0252e+00, 3.6721e+00, 9.0113e+00,\n",
      "        9.3533e-01, 4.1900e-01, 1.4387e-01, 9.2636e-01, 6.3801e-01, 1.5417e+01,\n",
      "        6.1909e-01, 3.0097e+00, 4.7362e-01, 4.1476e+00, 5.6851e-01, 2.5387e-01,\n",
      "        9.4864e-01, 1.1505e+00, 6.7147e-01, 7.7822e-01, 2.1138e-01, 8.5529e-02,\n",
      "        2.7532e+00, 2.2303e+01, 3.0278e+00, 1.7822e+01, 9.8245e-01, 3.2447e+00,\n",
      "        8.5194e-01, 2.0555e+00, 1.0958e+00, 1.2308e+00, 9.5641e-01, 4.7749e-01,\n",
      "        4.1829e-01, 3.9018e-01, 3.7296e-01, 1.9345e-01, 5.0529e-01, 1.8616e-01,\n",
      "        9.6581e-01, 7.5101e-01, 3.5089e-01, 2.4798e+00, 1.9293e-01, 3.7208e+00,\n",
      "        1.5913e-01, 7.6821e+00, 7.7334e-01, 1.2624e-01, 3.4488e+00, 5.8048e-01,\n",
      "        4.5784e+00, 2.5483e+00, 5.0560e+00, 4.2916e+00, 4.2134e-01, 9.0062e-01,\n",
      "        4.8408e-01, 8.6570e-02, 1.5759e-01, 3.2506e-01, 5.0407e-01, 1.1713e+00,\n",
      "        3.4684e+00, 7.6889e-01, 7.5087e+00, 9.7217e-01, 1.1773e+01, 6.8261e-01,\n",
      "        1.6593e+00, 1.1050e+01, 1.0557e+01, 1.7372e+00, 4.1972e+00, 2.4710e+00,\n",
      "        2.4090e+00, 4.0133e-01, 1.2416e-01, 6.4057e-01, 1.2600e+00, 1.0547e+00,\n",
      "        2.0695e+00, 6.5650e+00, 4.1355e-01, 5.6117e-01, 7.0621e-01, 2.1625e+00,\n",
      "        2.6000e-01, 2.8371e-01, 5.6291e-01, 4.2735e+00, 3.1306e-01, 2.7011e-01,\n",
      "        1.0504e+00, 1.7607e+00, 4.1042e+00, 1.7334e+00, 1.5375e+01, 1.1662e+01,\n",
      "        2.0119e+00, 3.1222e+00, 1.4476e+00, 4.6077e-01, 6.4215e-01, 1.8895e-01,\n",
      "        2.1564e-01, 3.1732e-01, 1.4663e-01, 2.0452e+00, 4.3344e-01, 6.5698e-01,\n",
      "        4.0668e-01, 7.2140e-03, 5.1938e+00, 1.0887e-01, 3.0273e-01, 6.7693e-01,\n",
      "        2.7633e+00, 1.1710e-01, 7.1660e+00, 8.1464e-02, 5.4881e-01, 1.8142e-01,\n",
      "        2.0436e+00, 1.4196e-01, 5.9034e-01, 1.0870e+00, 8.0374e-01, 8.1337e-01,\n",
      "        1.7056e-01, 6.1511e-01, 1.3276e+01, 2.3916e-01, 2.1347e+00, 1.7100e+00,\n",
      "        2.7526e+00, 4.1044e+00, 1.2946e+00, 8.8557e-02, 1.4590e+01, 4.1374e-01,\n",
      "        6.7542e-01, 1.3564e+00, 5.0211e-01, 8.6289e-01, 9.0432e-01, 1.6744e+00,\n",
      "        2.1713e+00, 7.0368e+00, 8.3195e-01, 6.3994e-02, 4.0592e-01, 5.6015e-01,\n",
      "        2.3868e-01, 1.5573e+01, 1.7573e-01, 3.0877e+00, 2.2279e+00, 4.3376e-01,\n",
      "        2.5645e+00, 8.1627e-01, 5.6047e-01, 1.6512e-01, 2.3065e-02, 2.9093e-01,\n",
      "        1.1899e-01, 6.3415e+00, 5.0667e-01, 1.9535e-01, 4.7976e+00, 1.2015e+01,\n",
      "        1.3500e+00, 4.7503e-01, 4.5140e-01, 8.7474e-01, 3.3413e+00, 7.0357e-01,\n",
      "        1.1558e+00, 1.8860e+00, 6.0117e-01, 7.3611e+00, 1.0069e+00, 4.3408e-01,\n",
      "        2.8564e-01, 3.0028e+00, 1.5317e-01, 3.7627e+00, 4.0009e-01, 1.0031e+00,\n",
      "        4.4241e-01, 3.7089e-01, 2.0709e+00, 6.9013e-01, 2.8668e+00, 2.5223e+00,\n",
      "        1.2049e-01, 1.3048e+00, 1.6192e-01, 3.5449e-01, 1.0559e+00, 4.2707e-01,\n",
      "        3.3280e-01, 2.8334e-01, 1.0826e+01, 1.0266e+00, 5.2343e-01, 7.7348e-01,\n",
      "        1.1725e+00, 4.6053e-01, 3.1040e+00, 1.5657e+00, 1.7424e-01, 8.6943e-01,\n",
      "        5.0543e+00, 2.1448e-01, 2.6909e+00, 4.8219e-01, 6.0093e+00, 2.9644e+00,\n",
      "        1.1258e-01, 1.2432e+00, 1.8876e-01, 5.6664e-01, 1.4170e+00, 2.5234e+00,\n",
      "        3.8574e+00, 1.8820e-01, 1.3891e+00, 3.0435e-01, 5.5533e+00, 7.1894e-01,\n",
      "        8.7690e-01, 1.5185e+01, 5.8554e-02, 2.0465e-01, 3.1628e-01, 1.1781e+00,\n",
      "        3.6394e-02, 3.9055e-01, 7.1765e+00, 2.3811e-01, 2.3688e+00, 6.9543e-01,\n",
      "        7.6373e-01, 5.7478e-01, 8.4979e-01, 6.3344e+00, 7.8746e-01, 3.4290e+00,\n",
      "        7.7802e-02, 1.7274e-01, 2.9639e+00, 7.9154e-01, 7.3151e+00, 6.8144e-01,\n",
      "        1.8424e-01, 1.4246e-01, 1.3822e+00, 7.1025e+00, 1.6628e-01, 3.3492e-01,\n",
      "        1.2735e-01, 5.3196e-01, 1.2879e-01, 2.5975e+00, 9.4967e+00, 7.9327e+00,\n",
      "        1.1509e+00, 1.2562e-01, 1.3949e+01, 2.5850e-01, 4.0543e-01, 9.0449e-01,\n",
      "        1.3298e+00, 2.7126e+00, 5.3443e-01, 1.3267e+00, 4.7611e-02, 4.6396e-01,\n",
      "        3.0928e+00, 2.5007e-01, 1.7066e-01, 9.9818e-02, 3.3785e+00, 2.8346e-01,\n",
      "        1.7022e-01, 4.8974e-01, 2.7995e-01, 1.8932e-01, 2.6549e+00, 1.5794e+00,\n",
      "        1.1888e+00, 2.5418e-01, 1.2455e+00, 5.2250e-01, 4.6733e-01, 7.1661e-01,\n",
      "        7.5872e-01, 2.8655e-01, 1.6094e-01, 8.3268e-01, 1.7366e+00, 1.6048e+00,\n",
      "        1.4089e+00, 1.5358e+00, 6.2540e+00, 1.9353e-01, 7.1971e-01, 2.4632e-01,\n",
      "        7.4682e-02, 1.0967e+00, 1.4702e+00, 5.7844e-01, 1.3199e+01, 1.0301e+00,\n",
      "        3.7389e-01, 1.1684e+00, 1.5102e+00, 6.6321e+00, 4.2244e-01, 2.4284e+00,\n",
      "        6.4812e-02, 3.4755e+00, 6.2642e-01, 1.7429e+00, 1.2141e+00, 5.6571e-01,\n",
      "        1.2031e+00, 6.7415e-01, 1.8763e-01, 1.7778e+01, 2.4881e-01, 8.6705e+00,\n",
      "        7.9555e-01, 4.3305e+00, 4.2135e-01, 1.5673e+00, 7.8357e-01, 1.0025e+00,\n",
      "        2.4337e+00, 1.9327e+01, 2.0069e-01, 2.1343e+00, 3.4326e+00, 3.2682e+00,\n",
      "        5.1363e-01, 5.5420e-01, 9.1028e-01, 7.4550e-02, 1.2382e+00, 1.0464e+00,\n",
      "        8.6411e-01, 2.2679e+00, 1.7112e-01, 3.7677e+00, 5.9384e-01, 4.1207e+00,\n",
      "        2.9940e-01, 6.4532e-01, 3.0314e-01, 9.4881e-02, 2.4543e-01, 1.8648e-01,\n",
      "        2.4906e+00, 1.2980e+00, 2.9083e+00, 2.2222e+00, 5.7058e-01, 1.2183e-01,\n",
      "        3.3551e-01, 8.6190e+00, 3.7104e-01, 4.7313e-01, 1.3094e+00, 1.3090e+00,\n",
      "        1.2645e-01, 1.4741e+00, 1.7640e+00, 5.4780e-02, 4.5524e-01, 3.1259e+00,\n",
      "        1.7334e+00, 2.1048e+00, 7.4269e-01, 1.0214e+01, 4.5237e-01, 8.3504e-02,\n",
      "        3.0318e+00, 1.0136e+00, 2.0737e+00, 1.2391e+00, 9.3591e-02, 2.4878e+01,\n",
      "        3.0938e-01, 1.1421e+00, 4.7144e-01, 3.8986e-01, 2.2126e+00, 5.2707e-01,\n",
      "        1.5910e+00, 2.4106e-01, 1.4864e+01, 9.6597e-01, 2.3109e+00, 5.0261e-01,\n",
      "        6.8387e-01, 6.9690e-02, 6.6498e-01, 1.4225e-01, 9.8475e-02, 1.2618e-01,\n",
      "        3.2916e-01, 6.2639e-02, 1.2544e+01, 5.3165e-01, 1.1579e+00, 3.3397e-01,\n",
      "        1.1688e-01, 5.4211e-01, 2.5636e-01, 3.1034e-01, 1.5409e+00, 4.9173e-01,\n",
      "        4.4723e-01, 1.1809e+01, 2.3738e-01, 4.7944e-01, 4.3185e-02, 1.0121e-01,\n",
      "        3.3151e-01, 3.3433e+00, 2.5639e+00, 1.1051e+00, 1.0678e-01, 8.7791e-01,\n",
      "        6.9201e-01, 1.4399e+01, 1.0916e+00, 6.6358e-01, 4.3792e+00, 4.8155e-01,\n",
      "        1.0257e+00, 3.7428e-01, 2.4024e+00, 4.4900e+00, 2.7521e+00, 2.1864e-01,\n",
      "        5.6400e-01, 7.2183e-01, 6.4460e-01, 2.4673e-01, 3.7083e-01, 3.5676e+00,\n",
      "        1.2972e+01, 3.1628e-01, 2.8224e+00, 5.0683e+00, 1.5687e+01, 3.4154e+00,\n",
      "        9.1714e-01, 2.9376e+00, 3.0442e-01, 1.2940e-01, 2.1484e+01, 1.8827e+00,\n",
      "        1.4161e+00, 1.8150e+00, 5.5693e-01, 4.8203e-01, 9.6279e-02, 1.6834e+01,\n",
      "        1.0841e+01, 5.6875e-01, 6.5267e-01, 8.7737e-02, 5.4760e-01, 1.2259e+01,\n",
      "        5.1470e-01, 2.7754e+00, 1.8726e-01, 1.3544e-01, 7.3358e-02, 7.1631e-01,\n",
      "        1.3541e+00, 7.3177e+00, 3.9045e-01, 1.2235e+00, 1.6833e-01, 1.2930e-01,\n",
      "        4.0065e+00, 8.7718e-01, 7.0210e-01, 3.0375e+00, 4.4237e+00, 1.3323e-01,\n",
      "        2.1974e+00, 1.8784e+00, 3.3802e-01, 1.4431e+00, 1.5828e+00, 1.6899e+00,\n",
      "        1.7393e-01, 1.7713e-01, 4.3729e-01, 2.0806e+00, 5.1308e-01, 2.4855e+00,\n",
      "        6.2401e-01, 1.6124e+00, 1.3041e-01, 1.0144e-01, 5.9543e-01, 6.5080e-01,\n",
      "        6.5557e-01, 7.5038e-01, 3.2417e-01, 1.8466e+01, 2.1722e+00, 8.9429e-02,\n",
      "        4.8637e+00, 1.6649e+00, 1.6125e+01, 5.6097e-01, 3.5473e-01, 1.3277e+00,\n",
      "        6.6387e-01, 4.5309e+00, 1.2868e-01, 7.0687e-01, 4.9870e-01, 1.5032e-01,\n",
      "        6.0616e-01, 7.9997e-01, 1.8770e+00, 1.3672e-01, 5.3302e-01, 5.6893e+00,\n",
      "        9.3052e-03, 1.9663e+00, 2.0717e+00, 9.0777e-02, 5.5689e+00, 3.3461e-01,\n",
      "        5.3650e+00, 3.2734e+00, 1.9721e+00, 2.3778e-01, 3.4069e+00, 2.5255e+00,\n",
      "        2.0576e+00, 1.5222e+00, 1.2938e-01, 1.6991e+00, 2.7063e+00, 7.1821e-01,\n",
      "        7.1284e+00, 5.9494e-01, 8.7318e-01, 1.5171e+00, 3.0842e+00, 2.8955e+00,\n",
      "        1.9798e-01, 5.5538e-01, 2.9977e+00, 3.7034e-01, 9.1311e-01, 7.8304e-01,\n",
      "        3.4706e-01, 4.6950e+00, 8.9050e-02, 1.2019e+00, 3.3878e-01, 1.1922e+00,\n",
      "        1.8854e-02, 1.0705e+00, 4.9279e-01, 5.9577e+00, 1.6585e+00, 1.4496e-01,\n",
      "        7.9005e-01, 7.8091e+00, 3.6064e-01, 3.7305e-01, 9.3036e-01, 4.0912e-01,\n",
      "        9.2256e+00, 3.3640e-01, 3.9189e-01, 2.7619e+00, 1.8450e+01, 2.4056e+00,\n",
      "        1.7434e-01, 5.7783e+00, 1.0009e+01, 1.0241e+00, 3.5293e-01, 7.6378e-02,\n",
      "        4.2304e+00, 3.4970e-01, 8.1983e-01, 5.1763e-01, 1.7458e+00, 5.7163e-01,\n",
      "        1.7348e-01, 2.4803e-01, 2.4986e-01, 2.8121e+00, 4.4407e+00, 2.8736e+00,\n",
      "        3.3458e+00, 9.3728e-01, 2.1218e+00, 1.0564e+00, 6.9105e+00, 1.2147e+01,\n",
      "        4.7182e-01, 5.7451e-01, 4.8810e-01, 2.1505e+00, 2.0098e+00, 5.3940e-01,\n",
      "        4.3736e-01, 8.5464e-01, 9.9970e-01, 7.7136e-01, 1.4482e+00, 1.5613e-01,\n",
      "        8.5772e-02, 2.7418e-01, 1.7835e+00, 4.5495e-01, 3.0242e+00, 5.1700e-01,\n",
      "        2.0637e+00, 5.6403e-01, 2.2300e-01, 1.9202e-01, 3.2473e+00, 6.0819e+00,\n",
      "        1.4216e-01, 2.0756e-01, 6.4922e+00, 1.3229e+00, 1.8173e-02, 1.7131e-01,\n",
      "        7.3562e+00, 3.4482e-01, 6.1880e-01, 6.4364e-01, 5.8384e-02, 3.3961e+00,\n",
      "        1.4654e+00, 1.9516e+00, 8.8756e-02, 6.4681e-02, 2.1110e+00, 9.6263e-01,\n",
      "        2.8376e-01, 4.1858e-01, 6.6940e-01, 1.1867e+00, 1.5029e+00, 4.4729e+00,\n",
      "        3.7378e-01, 4.3444e-01, 6.3430e-01, 9.7404e-01, 7.2311e-01, 3.3186e+00,\n",
      "        9.5907e-01, 1.0766e+01, 3.4804e-01, 3.1185e-01, 6.3274e-01, 1.6216e+01,\n",
      "        5.1551e-01, 2.3300e+00, 2.9897e+00, 4.2824e-02, 3.3570e-01, 1.9549e+00,\n",
      "        7.2246e-01, 3.6403e-01, 6.1593e-01, 6.0934e-01, 1.0293e+01, 8.3767e-02,\n",
      "        1.1185e+00, 1.1871e+00, 4.0174e+00, 5.8663e-01, 1.1539e+00, 4.4095e-01,\n",
      "        1.2391e+00, 2.1906e-01, 1.1324e+00, 1.0636e-01, 2.7028e+00, 2.1988e+00,\n",
      "        1.2860e+00, 7.1691e+00, 2.8906e-01, 1.0374e+01, 4.6962e+00, 3.6234e+00,\n",
      "        1.1594e+01, 1.1743e-01, 2.4903e+00, 8.5521e-01, 3.5916e-01, 7.4856e-01,\n",
      "        4.2854e+00, 4.3254e-01, 3.6442e-01, 1.8207e-01, 4.3350e-01, 3.5225e-01,\n",
      "        4.5849e-01, 2.7121e-01, 4.3538e-01, 1.9318e-01, 2.7017e+00, 1.2264e-01,\n",
      "        1.7175e+01, 1.2487e-01, 5.9029e-01, 4.7783e+00, 1.3115e-01, 1.6128e+00,\n",
      "        2.6266e-01, 5.8318e-01, 7.2835e+00, 1.1300e+01, 8.5918e-01, 7.9100e+00,\n",
      "        2.7356e+00, 7.7041e-01, 4.3930e-01, 1.1748e+01, 8.2066e-02, 7.2146e-02,\n",
      "        6.5704e-02, 1.8433e-01, 1.2775e+01, 2.7022e-01, 6.7725e-01, 9.4416e-02,\n",
      "        1.0108e+01, 1.1126e+00, 1.0154e+00, 9.2997e+00, 4.3433e+00, 1.4872e+00,\n",
      "        7.9515e-02, 2.9007e-01, 1.4113e+01, 4.3515e-01, 1.1900e-01, 5.6695e-01,\n",
      "        2.0832e+01, 7.9066e-01, 4.0686e+00, 8.1891e-01, 3.2070e-01, 6.8054e-01,\n",
      "        5.1956e-01, 1.2372e-01, 2.5726e+00, 1.3721e+00, 6.0979e+00, 1.2560e+01,\n",
      "        2.6541e-01, 4.7256e+00, 3.0597e-01, 7.6778e-01, 1.1656e+00, 7.4670e-01,\n",
      "        1.2372e-01, 1.0844e+00, 6.1942e-01, 7.0621e+00, 1.8223e-01, 8.3724e-01,\n",
      "        3.0197e-01, 1.6672e+01, 2.6092e+00, 6.0105e-01, 1.3984e+00, 2.4905e-01,\n",
      "        5.2758e-01, 1.4525e+00, 5.3583e-01, 1.7850e+00, 3.1843e-01, 4.3999e-02,\n",
      "        5.9592e-01, 1.6630e-01, 4.1706e-02, 1.0999e+00, 2.3223e-01, 2.8030e-01,\n",
      "        1.9816e+00, 2.6947e-01, 2.4145e+00, 6.7943e-01, 2.3942e+00, 4.4180e-01,\n",
      "        1.9586e+00, 2.8321e+00, 6.6078e-01, 6.3301e-01, 6.5188e+00, 1.3928e-01,\n",
      "        6.0688e-01, 3.6338e-01, 3.1753e-01, 3.9779e-01, 4.3292e+00, 5.9023e-01,\n",
      "        1.4862e+00, 7.0727e-01, 1.1524e-01, 5.6333e-01, 1.0186e+01, 8.1631e+00,\n",
      "        3.8173e-01, 1.1428e+00, 7.4024e-01, 5.8450e-01, 5.0103e-01, 2.0403e+00,\n",
      "        4.2248e+00, 8.1720e-01, 5.8162e+00, 5.8802e-01, 7.0628e-01, 1.0256e-01,\n",
      "        8.8287e-02, 1.7751e+00, 1.3801e+01, 1.5973e-01, 1.1041e+00, 4.2072e+00,\n",
      "        1.4435e-01, 1.3703e-01, 3.5966e-01, 5.7992e-02, 4.9403e-01, 1.2434e+00,\n",
      "        6.8204e-01, 7.2351e-01, 1.0881e+00, 4.6845e+00, 1.7556e-01, 5.2930e-01,\n",
      "        1.8838e+00, 1.6236e+00, 1.4628e+00, 2.0091e+00, 3.8656e-01, 1.8592e+01,\n",
      "        1.0993e-01, 1.7443e+01, 2.0060e+00, 1.3206e+00, 6.9329e-01, 7.7603e-01,\n",
      "        1.8214e+00, 1.0311e-01, 5.4293e+00, 5.1992e-02, 9.8644e-01, 2.5550e+00,\n",
      "        5.2520e-01, 2.1690e+00, 2.3906e-01, 4.3221e-01, 9.4886e+00, 5.0573e-01,\n",
      "        2.7182e-01, 1.5263e-01, 1.4946e-01, 3.3003e-01, 1.2696e+00, 1.0602e-01,\n",
      "        2.7153e-01, 2.6856e+00, 7.9881e-01, 2.2293e+01, 1.6597e+00, 8.6996e-02,\n",
      "        3.0783e+00, 3.2936e-01, 1.7038e+00, 1.3434e+00, 2.4165e+00, 2.9189e-01,\n",
      "        4.2890e-01, 1.1350e+00, 5.1534e-01, 2.9090e-01, 1.5414e+00, 2.3576e+00,\n",
      "        8.3701e-01, 1.9605e-01, 2.3640e+00, 1.9424e+00, 2.6846e-01, 1.9379e+00,\n",
      "        5.9650e-01, 1.4243e+00, 3.3777e+00, 1.4793e-01, 1.7409e+00, 1.0672e-01,\n",
      "        5.3789e-01, 3.6040e-01, 1.4757e+00, 1.3803e+00])\n"
     ]
    }
   ],
   "source": [
    "wrapped_model.eval()\n",
    "\n",
    "for data, target in test_loader:\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    output, unc = wrapped_model(data)\n",
    "    print(output.shape, unc.shape)\n",
    "    print(unc)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7.3582, 7.9131, 6.7250, 7.1332, 6.8550, 7.9214, 7.1222, 7.9870, 7.5219,\n",
      "        6.9595, 7.0359, 7.3568, 7.6546, 7.3099, 7.2918, 7.6565, 6.9031, 6.7241,\n",
      "        7.4198, 6.9781, 7.7382, 7.5573, 7.4055, 7.2572, 7.3463, 7.2791, 7.3249,\n",
      "        7.2255, 6.6497, 6.9350, 7.0896, 8.3361, 7.5479, 7.7242, 7.9768, 6.9496,\n",
      "        7.2433, 7.3700, 7.2580, 8.0888, 8.0518, 7.1969, 7.3811, 6.9334, 6.5396,\n",
      "        7.5903, 7.4960, 7.6984, 7.7670, 7.2601, 6.9021, 7.0287, 7.9619, 7.8759,\n",
      "        7.1458, 6.9842, 7.5583, 7.0821, 7.6115, 7.7466, 7.5403, 7.8372, 8.1009,\n",
      "        7.4912, 7.5474, 6.7825, 7.4189, 7.5562, 6.6698, 7.6832, 6.9256, 7.4270,\n",
      "        8.1530, 7.3856, 7.1763, 7.3068, 8.0920, 6.9831, 7.2108, 7.0776, 7.4730,\n",
      "        7.5372, 7.1913, 7.5240, 8.0783, 7.1096, 7.4955, 7.2483, 7.5776, 7.6372,\n",
      "        7.8779, 8.1441, 7.2814, 7.5347, 7.7365, 6.8730, 6.7651, 7.9368, 7.6651,\n",
      "        7.6879, 7.4084, 7.0093, 7.6615, 7.0789, 7.2043, 7.8899, 7.3410, 7.2440,\n",
      "        7.9782, 7.4676, 7.7927, 7.6790, 7.3679, 7.3621, 8.0382, 7.4070, 7.3802,\n",
      "        6.8989, 7.7844, 7.6154, 6.9486, 7.3291, 7.5738, 7.9418, 6.9410, 6.8789,\n",
      "        7.5539, 7.6053, 7.4280, 7.7782, 6.4070, 6.4977, 7.3688, 7.8972, 7.2564,\n",
      "        7.6027, 8.0028, 6.8633, 6.9099, 7.2871, 7.4411, 8.1466, 7.3389, 8.0383,\n",
      "        7.8252, 7.2445, 7.6581, 7.2087, 7.1012, 7.0312, 7.4815, 7.7652, 7.8166,\n",
      "        6.9287, 7.8256, 6.4829, 7.9006, 7.4208, 6.6916, 7.4812, 7.6468, 6.6571,\n",
      "        7.5341, 7.5728, 7.3027, 7.4515, 7.5827, 7.4932, 7.2988, 7.0388, 7.9272,\n",
      "        6.9259, 7.6751, 6.9022, 7.7200, 8.0562, 8.0839, 7.7196, 6.6244, 7.4199,\n",
      "        7.6109, 7.4526, 7.6494, 6.5286, 7.7296, 7.0180, 7.6903, 7.1402, 7.1173,\n",
      "        6.8237, 7.2437, 7.9757, 7.1692, 7.6195, 7.5118, 7.4161, 7.1604, 7.8458,\n",
      "        7.1319, 7.9398, 8.1819, 7.4871, 7.2977, 7.0174, 7.5397, 7.0134, 6.6492,\n",
      "        6.6492, 7.5217, 7.7430, 7.3633, 8.0757, 7.4830, 7.7025, 7.9996, 7.2710,\n",
      "        6.9588, 7.7004, 7.9125, 7.1786, 7.1199, 7.4381, 6.8745, 7.7825, 7.6730,\n",
      "        7.0777, 7.5203, 7.6565, 6.7217, 7.2875, 7.6801, 6.8599, 6.9940, 7.4130,\n",
      "        7.0335, 6.8159, 7.6938, 7.7315, 7.4978, 7.7300, 7.1656, 7.3899, 7.0830,\n",
      "        8.1185, 7.8558, 7.3842, 6.9604, 7.9324, 7.4971, 8.3957, 7.0252, 7.1496,\n",
      "        7.0495, 6.7751, 7.4234, 8.2404, 7.7182, 7.3819, 7.6168, 7.6126, 8.3763,\n",
      "        6.8410, 7.5069, 8.2167, 7.7798, 7.3327, 6.8067, 7.6831, 8.6124, 7.8955,\n",
      "        7.3626, 7.0066, 7.6154, 8.0961, 7.6186, 7.4955, 7.1334, 7.1948, 8.1321,\n",
      "        7.8204, 7.8624, 7.9623, 7.6659, 7.4626, 7.3459, 6.9075, 7.0898, 7.1729,\n",
      "        7.0989, 7.0835, 7.1353, 7.8569, 7.9063, 7.4440, 7.3671, 8.1817, 7.2324,\n",
      "        7.1957, 6.9760, 7.1595, 6.5567, 7.5155, 7.0984, 7.6333, 7.6645, 6.8792,\n",
      "        7.2680, 8.4295, 6.7675, 7.6210, 6.8082, 6.3105, 6.7035, 7.5551, 7.5839,\n",
      "        7.8441, 7.2827, 7.3631, 7.1322, 7.3757, 7.7136, 7.4724, 7.4755, 7.1739,\n",
      "        7.2358, 7.4202, 7.2390, 7.5080, 6.8268, 6.6087, 7.3608, 7.5559, 7.1604,\n",
      "        7.3688, 7.5309, 7.5942, 7.1225, 8.0644, 7.6791, 6.7465, 6.7105, 6.8220,\n",
      "        7.2503, 7.3662, 7.6640, 7.2914, 7.4406, 7.3306, 7.5388, 7.3843, 7.1350,\n",
      "        7.3737, 6.8042, 8.1815, 7.0837, 7.3081, 6.8799, 7.3191, 6.4303, 6.9063,\n",
      "        7.5690, 6.8133, 8.0524, 7.0285, 7.5511, 6.6346, 7.4869, 7.3653, 7.2690,\n",
      "        7.6750, 7.9473, 7.6165, 7.7445, 7.0385, 6.8950, 7.5126, 6.5610, 8.3023,\n",
      "        7.7804, 7.2867, 6.5266, 7.8463, 6.9219, 7.4658, 8.1613, 7.4549, 7.4164,\n",
      "        7.2601, 7.2012, 7.1820, 7.0732, 7.2207, 6.9786, 8.2553, 7.1556, 7.6444,\n",
      "        8.1095, 7.4159, 7.9871, 7.6477, 7.0236, 7.2651, 8.0326, 7.5429, 7.6680,\n",
      "        7.5543, 7.1506, 7.2713, 7.4024, 7.1574, 7.0764, 7.4440, 6.8228, 7.6163,\n",
      "        7.8302, 7.4378, 7.5170, 7.3663, 7.1893, 7.4407, 6.7928, 8.1180, 7.2272,\n",
      "        7.9374, 7.0321, 7.1075, 8.0040, 6.9037, 7.0452, 6.5801, 6.9471, 7.3632,\n",
      "        6.8316, 7.1986, 7.3609, 7.2072, 7.1557, 6.9400, 7.6737, 7.4643, 7.5325,\n",
      "        7.0356, 7.6020, 7.6929, 6.7749, 7.3491, 7.4486, 8.9517, 7.0471, 7.0171,\n",
      "        7.0964, 7.9319, 7.0506, 7.7325, 7.1251, 6.6863, 7.1068, 6.8049, 7.5635,\n",
      "        6.8867, 7.5553, 7.3248, 7.5800, 7.5621, 7.1790, 7.1653, 7.5766, 7.7710,\n",
      "        6.8467, 6.6481, 7.6080, 7.2253, 7.5002, 7.7959, 7.7731, 7.0853, 7.5238,\n",
      "        7.5005, 7.2212, 7.7122, 6.9886, 7.2654, 7.8423, 7.8434, 6.6692, 7.7122,\n",
      "        7.5667, 8.4959, 7.3777, 7.8111, 7.9414, 7.8830, 7.2126, 7.1929, 7.2620,\n",
      "        7.4205, 7.6219, 7.2754, 8.1702, 7.5821, 7.9391, 7.5653, 7.3821, 6.6906,\n",
      "        7.7863, 7.4753, 7.7099, 7.6871, 7.8762, 8.0578, 7.7215, 7.0145, 8.0320,\n",
      "        7.3850, 7.5631, 7.2681, 7.5300, 6.7973, 7.3741, 6.9915, 7.4280, 6.5429,\n",
      "        7.6379, 7.1489, 6.8783, 7.0077, 7.3388, 7.3285, 7.0606, 6.7490, 7.3922,\n",
      "        7.4946, 8.0925, 7.6851, 7.2983, 7.7583, 6.8900, 7.2860, 6.9678, 7.1660,\n",
      "        7.0992, 7.6922, 7.3364, 6.8851, 7.4755, 7.3599, 6.8359, 7.9140, 6.4177,\n",
      "        7.8910, 7.9022, 7.9832, 6.5564, 7.0195, 8.2653, 7.6384, 8.3135, 7.4394,\n",
      "        7.4991, 6.1099, 8.0432, 7.0160, 7.4866, 7.5732, 7.9764, 7.7741, 7.7601,\n",
      "        7.9324, 7.2670, 7.9896, 7.2872, 7.3651, 7.6446, 7.5388, 7.7230, 6.9117,\n",
      "        7.9550, 7.9363, 7.4147, 7.6485, 7.2387, 7.3394, 7.4913, 6.5407, 7.3339,\n",
      "        7.3292, 7.9381, 7.3836, 7.0304, 6.8442, 7.1448, 7.3445, 6.8214, 7.9098,\n",
      "        7.4782, 7.0015, 6.0065, 7.0243, 7.6497, 7.5924, 7.8455, 7.6198, 7.6684,\n",
      "        7.1421, 7.4444, 7.1684, 7.2685, 7.3540, 7.2666, 8.5221, 8.0981, 7.0875,\n",
      "        6.7535, 7.8279, 7.0337, 7.2892, 7.6095, 6.9105, 7.7798, 7.9277, 6.8990,\n",
      "        7.0611, 7.1438, 6.9807, 6.8471, 8.0177, 6.8648, 8.2807, 7.0178, 8.4941,\n",
      "        8.0167, 7.2044, 7.3088, 7.0886, 7.1201, 8.3687, 7.2702, 7.1727, 7.0836,\n",
      "        7.2930, 6.7282, 6.9648, 7.4203, 7.2379, 8.3455, 7.9150, 6.7523, 7.1915,\n",
      "        6.9515, 7.8215, 7.9103, 7.7522, 8.2400, 8.1196, 7.9042, 7.1357, 7.7858,\n",
      "        7.7044, 7.0353, 6.6656, 7.5565, 7.1504, 6.5398, 7.6051, 7.8324, 7.5147,\n",
      "        7.4847, 6.9786, 7.2389, 7.7467, 7.4595, 6.8880, 7.5120, 6.7241, 6.5466,\n",
      "        7.7961, 7.4014, 6.9487, 6.8646, 7.1883, 7.7374, 7.8753, 6.6933, 6.9443,\n",
      "        6.8959, 7.3118, 7.1501, 6.6141, 7.0979, 8.0447, 7.7169, 8.0916, 7.1324,\n",
      "        7.2470, 7.5562, 6.9410, 7.0485, 7.4436, 7.9485, 7.7252, 7.6943, 6.9710,\n",
      "        7.0307, 6.7626, 7.5683, 8.1608, 7.1373, 8.0411, 7.0884, 7.1845, 7.1984,\n",
      "        6.6534, 8.0270, 7.3551, 6.8833, 6.7417, 8.1253, 7.1726, 7.6503, 7.4716,\n",
      "        7.8528, 7.5569, 7.6542, 7.6230, 7.4526, 7.2534, 7.5290, 7.7215, 7.0855,\n",
      "        7.0583, 7.1506, 7.3110, 6.0586, 6.8645, 6.6201, 7.5591, 7.0930, 7.0050,\n",
      "        7.3350, 7.6452, 7.8007, 8.2239, 7.2111, 7.7658, 7.3828, 7.4855, 7.3663,\n",
      "        7.6443, 6.7258, 7.2576, 6.2871, 6.8964, 7.0893, 7.9014, 7.6326, 7.5990,\n",
      "        8.1437, 7.4621, 7.8786, 7.6650, 7.1895, 7.7339, 7.0618, 7.1983, 8.2600,\n",
      "        7.3674, 7.0980, 6.1905, 7.1641, 6.3712, 7.3091, 6.7909, 7.2156, 7.7209,\n",
      "        9.1983, 6.9050, 7.2835, 6.8543, 7.0425, 8.0638, 8.0703, 7.1939, 7.9483,\n",
      "        6.8504, 6.9816, 7.6863, 7.7817, 7.3437, 6.5404, 7.4881, 7.5597, 7.3031,\n",
      "        6.1915, 7.8238, 7.2448, 7.2762, 7.2845, 7.2173, 7.6708, 7.7139, 7.5910,\n",
      "        7.8083, 7.5260, 6.3854, 7.5988, 7.3813, 7.1612, 7.0236, 7.8519, 7.3494,\n",
      "        7.6688, 7.4967, 7.6930, 7.3168, 7.7581, 6.9435, 6.5058, 8.3564, 6.7386,\n",
      "        6.8806, 8.5376, 7.1998, 7.2357, 7.6471, 7.5898, 7.0427, 7.4042, 8.0173,\n",
      "        7.6099, 7.9855, 7.0010, 7.8007, 7.6082, 7.6044, 7.1932, 7.0128, 7.3751,\n",
      "        8.1711, 7.2486, 7.2474, 7.2841, 7.4098, 7.5914, 7.5632, 8.1900, 7.6098,\n",
      "        7.1630, 7.4647, 7.0718, 6.8991, 6.9447, 6.9607, 7.5776, 6.2582, 7.4146,\n",
      "        7.1936, 7.9329, 7.0822, 6.0976, 7.1498, 8.2246, 7.6845, 7.8871, 8.4116,\n",
      "        7.1646, 7.8885, 7.4517, 7.1724, 7.4616, 8.0899, 7.4146, 6.7541, 6.8908,\n",
      "        6.9223, 7.9282, 7.2082, 7.1297, 7.1130, 7.1168, 7.1166, 7.1858, 7.6136,\n",
      "        7.0666, 7.8655, 7.0195, 6.9683, 7.2698, 8.5069, 7.4223, 7.2569, 7.5168,\n",
      "        7.2670, 6.8668, 7.5591, 7.1521, 6.7565, 7.8673, 7.0635, 8.3885, 7.8350,\n",
      "        8.0497, 8.0450, 7.6375, 7.4708, 7.3481, 7.0496, 8.2336, 7.3682, 7.4225,\n",
      "        7.8320, 6.4797, 6.3573, 7.4618, 7.5490, 6.7690, 8.3622, 7.1718, 7.5120,\n",
      "        8.4900, 6.8556, 6.9078, 7.9650, 7.7652, 8.2562, 7.0194, 7.7346, 7.5893,\n",
      "        8.0320, 6.8720, 7.8500, 7.1829, 6.3969, 7.5419, 7.4275, 7.2387, 7.6595,\n",
      "        7.7549, 7.2972, 8.6242, 7.7950, 7.8006, 8.1214, 7.4274, 7.2271, 7.8922,\n",
      "        7.0932, 6.4852, 6.9152, 7.2540, 7.4875, 6.6042, 7.2970, 6.3540, 8.1185,\n",
      "        7.8285, 7.2131, 7.2687, 7.7208, 6.7099, 6.7011, 7.2535, 7.7834, 7.7041,\n",
      "        7.0776, 6.5508, 7.1806, 7.0789, 6.8892, 8.2806, 7.3712, 7.8973, 7.2158,\n",
      "        7.6669, 8.3732, 7.3669, 7.6339, 7.3303, 6.9403, 7.3322, 7.9491, 6.8484,\n",
      "        7.8507, 7.2608, 7.8571, 7.3258, 7.0056, 7.2381, 6.9777, 7.4559, 7.6712,\n",
      "        7.4061, 7.9259, 7.4353, 7.8904, 7.1780, 6.3668, 7.4979, 6.9482, 8.2282,\n",
      "        7.7468])\n"
     ]
    }
   ],
   "source": [
    "# Randomly set a batch\n",
    "data = torch.rand(1000, 1, 28, 28).to(device)\n",
    "output_r, unc_r = wrapped_model(data)\n",
    "print(unc_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(978)\n",
      "tensor(904)\n"
     ]
    }
   ],
   "source": [
    "threshold = 6.5\n",
    "print((unc_r > threshold).sum())\n",
    "print((unc < threshold).sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs5340",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
