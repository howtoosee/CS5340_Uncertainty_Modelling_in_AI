{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General flow of work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### setting dir as root, sys path append\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('/Users/xy/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Implementations/CS5340_Uncertainty_Modelling_in_AI')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import configs\n",
    "\n",
    "Be sure to import ood config and the benchmark dataset config. \n",
    "Taking MNIST as example: mnist.yml and mnist_ood.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in config (used to pass as config file compiled in command line)\n",
    "import yaml\n",
    "from openood.utils import Config\n",
    "from openood.utils.config import merge_configs\n",
    "\n",
    "# load in ood config and base dataset config\n",
    "with open(\"/Users/xy/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Implementations/CS5340_Uncertainty_Modelling_in_AI/configs/datasets/mnist/mnist_ood.yml\", \"r\") as yaml_file:\n",
    "    mnist_ood_config = Config(yaml.safe_load(yaml_file))\n",
    "\n",
    "with open(\"/Users/xy/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Implementations/CS5340_Uncertainty_Modelling_in_AI/configs/datasets/mnist/mnist.yml\", \"r\") as yaml_file:\n",
    "    mnist_config = Config(yaml.safe_load(yaml_file))\n",
    "\n",
    "## passing thru custom config function to merge and output config\n",
    "    mnist_config_merged = merge_configs(mnist_config, mnist_ood_config)\n",
    "\n",
    "###### CIFAR 10 ######\n",
    "# load in ood config and base dataset config\n",
    "with open(\"/Users/xy/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Implementations/CS5340_Uncertainty_Modelling_in_AI/configs/datasets/cifar10/cifar10.yml\", \"r\") as yaml_file:\n",
    "    cifar10_ood_config = Config(yaml.safe_load(yaml_file))\n",
    "\n",
    "with open(\"/Users/xy/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Implementations/CS5340_Uncertainty_Modelling_in_AI/configs/datasets/cifar10/cifar10_ood.yml\", \"r\") as yaml_file:\n",
    "    cifar10_config = Config(yaml.safe_load(yaml_file))\n",
    "\n",
    "## passing thru custom config function to merge and output config\n",
    "    cifar10_config_merged = merge_configs(cifar10_config, cifar10_ood_config)\n",
    "\n",
    "###### CIFAR 100 ######\n",
    "# load in ood config and base dataset config\n",
    "with open(\"/Users/xy/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Implementations/CS5340_Uncertainty_Modelling_in_AI/configs/datasets/cifar100/cifar100.yml\", \"r\") as yaml_file:\n",
    "    cifar100_ood_config = Config(yaml.safe_load(yaml_file))\n",
    "\n",
    "with open(\"/Users/xy/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Implementations/CS5340_Uncertainty_Modelling_in_AI/configs/datasets/cifar100/cifar100_ood.yml\", \"r\") as yaml_file:\n",
    "    cifar100_config = Config(yaml.safe_load(yaml_file))\n",
    "\n",
    "## passing thru custom config function to merge and output config\n",
    "    cifar100_config_merged = merge_configs(cifar100_config, cifar100_ood_config)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Config.all_keys of dataset:\n",
       "    image_size: 32\n",
       "    interpolation: bilinear\n",
       "    name: cifar10\n",
       "    normalization_type: cifar10\n",
       "    num_classes: 10\n",
       "    num_gpus: @{num_gpus}\n",
       "    num_machines: @{num_machines}\n",
       "    num_workers: @{num_workers}\n",
       "    pre_size: 32\n",
       "    split_names: ['train', 'val', 'test']\n",
       "    test:\n",
       "        batch_size: 200\n",
       "        data_dir: ./data/images_classic/\n",
       "        dataset_class: ImglistDataset\n",
       "        imglist_pth: ./data/benchmark_imglist/cifar10/test_cifar10.txt\n",
       "        shuffle: False\n",
       "    train:\n",
       "        batch_size: 128\n",
       "        data_dir: ./data/images_classic/\n",
       "        dataset_class: ImglistDataset\n",
       "        imglist_pth: ./data/benchmark_imglist/cifar10/train_cifar10.txt\n",
       "        shuffle: True\n",
       "    val:\n",
       "        batch_size: 200\n",
       "        data_dir: ./data/images_classic/\n",
       "        dataset_class: ImglistDataset\n",
       "        imglist_pth: ./data/benchmark_imglist/cifar10/val_cifar10.txt\n",
       "        shuffle: False\n",
       "ood_dataset:\n",
       "    batch_size: 128\n",
       "    dataset_class: ImglistDataset\n",
       "    farood:\n",
       "        datasets: ['mnist', 'svhn', 'texture', 'place365']\n",
       "        mnist:\n",
       "            data_dir: ./data/images_classic/\n",
       "            imglist_pth: ./data/benchmark_imglist/cifar10/test_mnist.txt\n",
       "        place365:\n",
       "            data_dir: ./data/images_classic/\n",
       "            imglist_pth: ./data/benchmark_imglist/cifar10/test_places365.txt\n",
       "        svhn:\n",
       "            data_dir: ./data/images_classic/\n",
       "            imglist_pth: ./data/benchmark_imglist/cifar10/test_svhn.txt\n",
       "        texture:\n",
       "            data_dir: ./data/images_classic/\n",
       "            imglist_pth: ./data/benchmark_imglist/cifar10/test_texture.txt\n",
       "    name: cifar10_ood\n",
       "    nearood:\n",
       "        cifar100:\n",
       "            data_dir: ./data/images_classic/\n",
       "            imglist_pth: ./data/benchmark_imglist/cifar10/test_cifar100.txt\n",
       "        datasets: ['cifar100', 'tin']\n",
       "        tin:\n",
       "            data_dir: ./data/images_classic/\n",
       "            imglist_pth: ./data/benchmark_imglist/cifar10/test_tin.txt\n",
       "    num_classes: 10\n",
       "    num_gpus: @{num_gpus}\n",
       "    num_machines: @{num_machines}\n",
       "    num_workers: @{num_workers}\n",
       "    shuffle: False\n",
       "    split_names: ['val', 'nearood', 'farood']\n",
       "    val:\n",
       "        data_dir: ./data/images_classic/\n",
       "        imglist_pth: ./data/benchmark_imglist/cifar10/val_tin.txt>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar10_config_merged.all_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/benchmark_imglist/cifar100/train_cifar100.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m config_name_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcifar100\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcifar10\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmnist\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     11\u001b[0m ood_dataloader_dict \u001b[38;5;241m=\u001b[39m {config_name_list[pt]: get_ood_dataloader(config_list[pt], DATA_PATH) \u001b[38;5;28;01mfor\u001b[39;00m pt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(config_list))}\n\u001b[0;32m---> 12\u001b[0m dataloader_dict \u001b[38;5;241m=\u001b[39m {config_name_list[pt]: \u001b[43mget_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDATA_PATH\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m pt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(config_list))}\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Implementations/CS5340_Uncertainty_Modelling_in_AI/openood/datasets/utils.py:86\u001b[0m, in \u001b[0;36mget_dataloader\u001b[0;34m(config, data_dir)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     85\u001b[0m     CustomDataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(split_config\u001b[38;5;241m.\u001b[39mdataset_class)\n\u001b[0;32m---> 86\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mCustomDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimglist_pth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimglist_pth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreprocessor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreprocessor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_aux_preprocessor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_aux_preprocessor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     sampler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;66;03m# if dataset_config.num_gpus * dataset_config.num_machines > 1:\u001b[39;00m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;66;03m#     sampler = torch.utils.data.distributed.DistributedSampler(\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;66;03m#         dataset)\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;66;03m#     split_config.shuffle = False\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Implementations/CS5340_Uncertainty_Modelling_in_AI/openood/datasets/imglist_dataset.py:38\u001b[0m, in \u001b[0;36mImglistDataset.__init__\u001b[0;34m(self, name, imglist_pth, data_dir, num_classes, preprocessor, data_aux_preprocessor, maxlen, dummy_read, dummy_size, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28msuper\u001b[39m(ImglistDataset, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m name\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimglist_pth\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m imgfile:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimglist \u001b[38;5;241m=\u001b[39m imgfile\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir \u001b[38;5;241m=\u001b[39m data_dir\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/benchmark_imglist/cifar100/train_cifar100.txt'"
     ]
    }
   ],
   "source": [
    "## setting up data directory\n",
    "\n",
    "from openood.datasets.utils import get_ood_dataloader \n",
    "from openood.datasets.utils import get_dataloader \n",
    "\n",
    "\n",
    "DATA_PATH = '/Users/xy/Downloads/data' # replace with ur data dir path here\n",
    "\n",
    "config_list = [cifar100_config_merged, cifar10_config_merged, mnist_config_merged]\n",
    "config_name_list = ['cifar100', 'cifar10', 'mnist']\n",
    "ood_dataloader_dict = {config_name_list[pt]: get_ood_dataloader(config_list[pt], DATA_PATH) for pt in range(len(config_list))}\n",
    "dataloader_dict = {config_name_list[pt]: get_dataloader(config_list[pt], DATA_PATH) for pt in range(len(config_list))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val': <torch.utils.data.dataloader.DataLoader at 0x29b209be0>,\n",
       " 'nearood': {'cifar100': <torch.utils.data.dataloader.DataLoader at 0x29b2086e0>,\n",
       "  'tin': <torch.utils.data.dataloader.DataLoader at 0x29b209490>},\n",
       " 'farood': {'mnist': <torch.utils.data.dataloader.DataLoader at 0x29b2090d0>,\n",
       "  'svhn': <torch.utils.data.dataloader.DataLoader at 0x29b209ac0>,\n",
       "  'texture': <torch.utils.data.dataloader.DataLoader at 0x29b208f80>,\n",
       "  'place365': <torch.utils.data.dataloader.DataLoader at 0x29b20a600>}}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar10_datalaoder = dataloader_dict['cifar10']\n",
    "cifar10_datalaoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_dataset = cifar10_datalaoder['val'].dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
